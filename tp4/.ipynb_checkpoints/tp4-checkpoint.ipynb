{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of this code is freely inspired from the torch and pytorch architectures, and some sample of code are re-used. It is however designed to be as concise as possible, not in any case for efficiency or flexibility (i.e. if you ever want to do Deep Learning out of this class, don't try to adapt this code, use an existing framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. A simple MLP implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always some useful imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    "import scipy.ndimage as ndimage\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # tqdm is a very useful library to monitor the progression of your loops\n",
    "    # (not necessary but handy)\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a Neural Network as a modular architecture, all layer and loss being a `Module`. We define the `Linear`, `ReLU` layers, necessary to implement a MLP, as well as a simple 2-layer MLP and the least square loss function, `LeastSquareCriterion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self.gradInput = None \n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        Should be overriden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *input):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "        Should be overriden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LeastSquareCriterion(Module):\n",
    "    \"\"\"\n",
    "    This implementation of the least square loss assumes that the data comes as a 2-dimensional array\n",
    "    of size (batch_size, num_classes) and the labels as a vector of size (num_classes) \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeastSquareCriterion, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        target = np.zeros([x.shape[0], self.num_classes])\n",
    "        for i in range(x.shape[0]):\n",
    "            target[i, labels[i]] = 1\n",
    "        self.output = np.sum((target-x)**2, axis=1)\n",
    "        return np.mean(self.output)\n",
    "    \n",
    "    def backward(self, x, labels):\n",
    "        self.gradInput = x\n",
    "        for i in range(x.shape[0]):\n",
    "            self.gradInput[i, labels[i]] = x[i, labels[i]]-1\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    The input is supposed to have two dimensions (batch_size, in_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = (\n",
    "            math.sqrt(1. / (out_features*in_features)) * \n",
    "            np.random.randn(out_features, in_features)\n",
    "        )\n",
    "        self.bias = np.zeros(out_features)\n",
    "        \n",
    "        self.gradWeight = None\n",
    "        self.gradBias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.output = np.dot(x, self.weight.transpose()) + self.bias[None, :]\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        self.gradInput = np.dot(gradOutput, self.weight)\n",
    "        self.gradWeight = np.dot(gradOutput.transpose(), x)\n",
    "        self.gradBias = np.sum(gradOutput, axis=0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def gradientStep(self, lr):\n",
    "        self.weight = self.weight - lr*self.gradWeight\n",
    "        self.bias = self.bias - lr*self.gradBias\n",
    "        \n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self, bias=True):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.output = x.clip(0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        self.gradInput = (x>0) * gradOutput\n",
    "        return self.gradInput\n",
    "\n",
    "\n",
    "class SimpleMLP(Module):\n",
    "    \"\"\"\n",
    "    This class is a simple example of a neural network, composed of two\n",
    "    linear layers, with a ReLU non-linearity in the middle\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dimension=784, num_classes=10):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = Linear(in_dimension, 64)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x, gradient):\n",
    "        gradient = self.fc2.backward(self.relu1.output, gradient)\n",
    "        gradient = self.relu1.backward(self.fc1.output, gradient)\n",
    "        gradient = self.fc1.backward(x, gradient)\n",
    "        return gradient\n",
    "    \n",
    "    def gradientStep(self, lr):\n",
    "        self.fc2.gradientStep(lr)\n",
    "        self.fc1.gradientStep(lr)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network, we will need data. Download the Mini-MNIST data (on Moodle) (~4Mo). I consists of 28Ã—28 images (loaded as vectors of size 784) and the associated label for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load(\"mini_mnist.npz\")\n",
    "\n",
    "train_data = data[\"train_data\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "test_data = data[\"test_data\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "N_train = train_data.shape[0]\n",
    "N_test = test_data.shape[0]\n",
    "\n",
    "# Check that data makes sense\n",
    "plt.figure()\n",
    "plt.imshow(train_data[0, :].reshape(28,28))\n",
    "print(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and understand the code provided above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.a.** Write a function `train_iter(model, loss, batch_data, batch_labels, lr)` that trains one iteration of a given `model` and `loss` (subclasses of `Module`) on a batch `(batch_data, batch_labels)` with a learning rate `lr`, and returns the training loss value. <br/>\n",
    "**b.** Test your function: write a simple loop that trains for 50 iterations a `SimpleMLP` on `train_data` with a learning rate `1e-3` and batches of size `16`.\n",
    "\n",
    "*Note: An **iteration** consists in running one batch through the model; an **epoch** consists in running multiple iterations so as to process each example of the training set once.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**2.a.** Write a function `evaluate(model, loss, data, labels)` that returns a couple `(loss, accuracy)` corresponding to the loss and accuracy computed on the whole `data`. <br/>\n",
    "**b.** Use your function to evaluate the model trained in question **1.b.** on the training and testing data. <br/> \n",
    "**c.** Check visually the quality of the results by plotting a few examples and their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**3.** Write a loop that trains a `SimpleMLP` for a whole epoch with the same parameters as in question **1.a.**. Plot the training and testing losses and accuracies during this training. \n",
    "\n",
    "*Note: you don't have to evaluate your network at every iteration, you can for example do it every 10 iterations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.a.** Based on the previous questions, design a function \n",
    "\n",
    "<center><pre>train(model, loss, train_data, train_labels, test_data, test_labels, ...)</pre></center>\n",
    "\n",
    "that performs a full training of the input `model` on the training data. Choose the arguments of your function (for instance, `batch_size`, `lr`, `epochs`, etc) according to what seems relevant to you. This function should plot the training and validation losses and accuracies during training. <br/>\n",
    "**b.** Use your function to train a model with a good accuracy (decide for a learning rate, batch size...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** What's the number of parameters in the network `SimpleMLP` used in the first section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.a.** Modify the definition of `SimpleMLP` to allow changing the dimension of the hidden layer. <br/>\n",
    "**b.** Define a class `DoubleMLP` with three `Linear` layers, and parameters to change their dimensions. <br/>\n",
    "**c. (optional)** Define a class `DeepMLP` with a **list** parameter `hidden_features` that allows to vary the number of intermediate linear layers, and their dimensions:\n",
    "\n",
    "- `DeepMLP([64])` should be equivalent to `SimpleMLP()`;\n",
    "- `DeepMLP([64, 64, 64])` should be a MLP with four linear layers...\n",
    "\n",
    "**d.** Test your new classes: train a `DoubleMLP(64, 32)` and plot its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different sizes of networks and layers. <br/> \n",
    "**3.a.** Find a set of parameters and hyperparameters that shows strong evidence of overfitting. <br/>\n",
    "**b.** What's the number of parameters of this network? Why does it overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Add to `Linear` and one of your MLP classes an optional parameter for (L2) weight decay (in the `gradientStep` functions) and show that it can reduce overfitting in some cases.\n",
    "\n",
    "*(Note: improving the validation performance may be hard; you can look at a setting with only very little training data to see, and focus on showing that the gap between training and validation performance decreases)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following questions are optional and do not have a specific order! A limited number of bonus points can be granted on completing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Implement and test different non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Implement and test different loss functions (L1, Cross-Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Add some momentum to your training procedure. How does it change the training behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** (longer/harder) Implement and test layers that consider the 2D structure of the input data (max pooling, convolution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
